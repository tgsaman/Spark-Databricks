{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following file exists in the Databricks file system:\n",
    "/FileStore/sample/sandp_stocks.csv\n",
    "You can view a version of the file here:\n",
    "https://gitlab.com/opstar/share20/-/raw/master/sandp_stocks.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the spark.read method to create a dataframe from the file.\n",
    "\n",
    "Each tuple consists of daily market prices and volumes for stocks for a 5 year period.\n",
    "The stock name is identified by the letters in the column on the right (e.g. AAL is\n",
    "American Airlines).\n",
    "\n",
    "The dataframe should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "+----------+-----+-----+-----+-----+--------+----+\n",
    "|      date| open| high|  low|close|  volume|name|\n",
    "+----------+-----+-----+-----+-----+--------+----+\n",
    "|2013-02-08|15.07|15.12|14.63|14.75| 8407500| AAL|\n",
    "|2013-02-11|14.89|15.01|14.26|14.46| 8882000| AAL|\n",
    "|2013-02-12|14.45|14.51| 14.1|14.27| 8126000| AAL|\n",
    "|2013-02-13| 14.3|14.94|14.25|14.66|10259500| AAL|\n",
    "+----------+-----+-----+-----+-----+--------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = spark.read\\\n",
    " .format('csv')\\\n",
    " .option('header', 'true')\\\n",
    " .option('inferSchema','false')\\\n",
    " .load('/FileStore/tables/sandp_stocks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert the columns to some appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = spr.selectExpr( \\\n",
    "                    'date(date)'\\\n",
    "                   ,'float(open)'\\\n",
    "                   ,'float(high)'\\\n",
    "                   ,'float(low)'\\\n",
    "                   ,'float(close)'\\\n",
    "                   ,'int(volume)'\n",
    "                   ,'name'\n",
    "                   ,'year(date) AS year')\n",
    "\n",
    "spr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use groupBy to aggregate the data and calculate the total volume for each stock (name). The finished result should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "+----+-----------+\n",
    "|name|sum(volume)|\n",
    "+----+-----------+\n",
    "|ALXN| 2218905439|\n",
    "| AIV| 1509190118|\n",
    "| AVY|  957891856|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use groupBy to aggregate the data and calculate the total volume for each stock\n",
    "f = spr.groupBy('name').agg(sum('volume'))\n",
    " \n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now calculate the average closing stock price for Microsoft (MSFT) for each year.\n",
    "The finished result should look like this:\n",
    "avclose.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "+----+--------+\n",
    "|year|avgclose|\n",
    "+----+--------+\n",
    "|2013| 33.0902|\n",
    "|2014| 42.4533|\n",
    "|2015|46.71380|\n",
    "|2018| 90.2523|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = spr.where(spr.name=='MSFT').groupBy('year').agg(\n",
    "    avg('close').alias('avgclose')).orderBy('year')\n",
    " \n",
    "g.show()\n",
    "\n",
    "\n",
    "\n",
    "# Alternative version reading the year from the date string\n",
    "g = spr.where(spr.name=='MSFT')\\\n",
    "    .groupBy(spr.date[0:4].alias('year')).agg(\n",
    "        avg('close').alias('avgclose')).orderBy('year')\n",
    " \n",
    "g.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example of the alternative dictionary syntax for the aggregate\n",
    "#  -- no need to import the function but you can't so easily alias the aggregated column\n",
    "g = spr.where(spr.name=='MSFT')\\\n",
    "    .groupBy('year')\\\n",
    "    .agg({'close':'avg','open':'avg'})\\\n",
    "    .orderBy('year')\n",
    " \n",
    "g.show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "g.orderBy(g.year.desc(), g.avgclose.asc()).show()\n",
    " \n",
    "g.orderBy(col('year').desc(), col('avgclose').asc()).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
